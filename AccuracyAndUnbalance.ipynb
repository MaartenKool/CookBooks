{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Accuracy' and unbalanced data sets\n",
    "\n",
    "'Accuracy' as a metric seems to be attractive to evaluate the performance of a classifier. It tells you in what percentage of cases the classifier predicts correct overall. However accuracy can be deceiving, or downright misleading, when a dataset is not balanced.  \n",
    "Consider that we are evaluating a cancer diagnosis prediction. Cancer is in fact not very common in the sense that 97% of e.g. the dutch population actually was *not* diagnosed with cancer in the past 10 years'. So pretend we did a survey of the dutch population to test our diagnosis prediction. In this case we found 95 people without cancer and 5 with cancer. Our classifier identified 3 cancer cases in total: 2 False positive and 1 True positive. **spoiler-alert** This is a terrible classifier. Not only did it get 2 out of 3 wrong, of the people with actual cancer only 1 was correctly identified.  \n",
    "So intuitively this classifier should be dismissed as lethal quackery. However the accuracy is 0.94 or 94% which most people will deem to be very acceptable.\n",
    "\n",
    "3 things should be done to prevent this from happening:\n",
    "\n",
    "1. Compare to a baseline: in this case the majority class. I.e.with a baseline of 95%, an accuracy of 94% actually shows a $loss$ of information.\n",
    "2. The accuracy is this high because that classifier gets 93 of the 95 \" $wrong$ \" ones \"$right$\". Use recall instead; this measures the fraction of actual bad news cases it gets right. Or use preciseness which measures the fraction of predicted bad news cases it gets right\n",
    "3. Balance the dataset (upsampling or downsampling)  \n",
    "\n",
    "'(https://www.volksgezondheidenzorg.info/onderwerp/kanker/cijfers-context/huidige-situatie#node-prevalentie-van-kanker)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "y_true = []\n",
    "[y_true.append(0) for i in range(0,95)]\n",
    "for i in range(0,5):\n",
    "    y_true.append(1) \n",
    "y_pred = [0] * 93\n",
    "for i in range(0,2):\n",
    "    y_pred.append(1)\n",
    "for i in range(0,4):\n",
    "    y_pred.append(0)\n",
    "for i in range(0,1):\n",
    "    y_pred.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy score:0.94\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_benign</th>\n",
       "      <th>predicted_malign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_benign</th>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_malign</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_benign  predicted_malign\n",
       "true_benign                93                 2\n",
       "true_malign                 4                 1"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate accuracy and create confusion matrix\n",
    "df = pd.DataFrame()\n",
    "df['X'] = y_pred\n",
    "\n",
    "df['y_train'] = y_true\n",
    "\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "print(f'Accurcy score:{accuracy_score(y_true, y_pred)}')\n",
    "pd.DataFrame(confusion, \n",
    "             columns=['predicted_benign','predicted_malign'], \n",
    "             index=['true_benign','true_malign'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     benign       0.96      0.98      0.97        95\n",
      "     malign       0.33      0.20      0.25         5\n",
      "\n",
      "avg / total       0.93      0.94      0.93       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, labels=None, target_names=['benign', 'malign'], sample_weight=None, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 2, 4, 1)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion.ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score choosing most frequent: 0.95\n"
     ]
    }
   ],
   "source": [
    "#Create Dummy classifer Always Predicts The Modal Value Of Target\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create dummy classifer\n",
    "dummy = DummyClassifier(strategy='most_frequent', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "dummy.fit(df[['X']],df['y_train'])\n",
    "\n",
    "# Get accuracy score\n",
    "print(f'Accuracy score choosing most frequent: {dummy.score(X, y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a balanced dataset manually\n",
    "y_true = []\n",
    "[y_true.append(0) for i in range(0,50)]\n",
    "for i in range(0,50):\n",
    "    y_true.append(1) \n",
    "y_pred = [0] * 49\n",
    "for i in range(0,1):\n",
    "    y_pred.append(1)\n",
    "for i in range(0,40):\n",
    "    y_pred.append(0)\n",
    "for i in range(0,10):\n",
    "     y_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:0.59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_benign</th>\n",
       "      <th>predicted_malign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_benign</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_malign</th>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             predicted_benign  predicted_malign\n",
       "true_benign                49                 1\n",
       "true_malign                40                10"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate accuracy and create confusion matrix\n",
    "\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "print(f'Accuracy score:{accuracy_score(y_true, y_pred)}')\n",
    "pd.DataFrame(confusion, \n",
    "             columns=['predicted_benign','predicted_malign'], \n",
    "             index=['true_benign','true_malign'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     benign       0.55      0.98      0.71        50\n",
      "     malign       0.91      0.20      0.33        50\n",
      "\n",
      "avg / total       0.73      0.59      0.52       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\n",
    "classification_report(y_true, y_pred, labels=None, target_names=['benign', 'malign'],sample_weight=None, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 1, 40, 10)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion.ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
