{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display floats with 2 decimal places\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    " \n",
    "# Expand display limits\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" pathname \"  # defaults to you current directory\n",
    "df = pd.read_csv(data, na_values=['?']\n",
    "                 )  # there are load of options here, like\\\n",
    "# filling na's\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "# EXCEL ALERT!!!!: consider reviewing the data in a spreadsheet\n",
    "# first\n",
    "\n",
    "Signature: pd.read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None,\n",
    "                       dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='\"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=None, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=None, compact_ints=None, use_unsigned=None, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['col1'] = list1\n",
    "df['col2'] = list2\n",
    "df['col3'] = 12345\n",
    "df['col4'] = 'abcd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unfamiliar with the data lineage consider shuffling it(don't forget (to remember) the random seed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often a good idea to clean the columnames: (eg for Patsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basically removes anything that is not a letter, space or underscore.\n",
    "import string\n",
    "# the string library has default strings that contain all letters or numbers\n",
    "uppercase = string.ascii_uppercase\n",
    "lowercase = string.ascii_lowercase\n",
    "df.columns = [''.join([ch for ch in col if ch in uppercase+lowercase+' _']) \\\n",
    "              for col in df.columns]\n",
    "# for each character in each column name join characters together \n",
    "# if they are in the string \"uppercase+lowercase+' _'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace spaces in column names and convert all columns to lowercase:\n",
    "df.columns = [x.lower().replace(' ','_') for x in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces spaces with underscores.\n",
    "df.columns = map(lambda x: x.replace(' ', '_'), df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of unneeded columns. Columns not needed in the analysis at hand static data AND columns that are duplicates or closely alike: e.g. 'DistanceInMiles' and 'DistanceInKilometers' (MLR doesn't like that for one:-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicit colums to be deleted\n",
    "df.drop(['col1', 'col2'], axis =1, inplace = True)\n",
    "#or redefine df with the columns you want\n",
    "df = df[['col3', 'col4']] #!! [[]] !! not ([])\n",
    "#rename FIRST if your need to retain the old one\n",
    "df2 = df[['col3', 'col4']]\n",
    "#Creativity is also possible\n",
    "df = [[col for col in df.columns if 'keyword' in col\\\n",
    "       and 'keyword' not in col]] #!![[]]!!\n",
    "List = [col for col in df.columns if 'keyword' in col\\\n",
    "       and 'keyword' not in col]# single []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if part of a pipe line, avoid getting a failure\n",
    "def dropcol(df):\n",
    "        for col in['col1','col2', 'col3']:\n",
    "            try:\n",
    "                df.drop(col, axis=1, inplace=True)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be a good time to combine/convert columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['colNEW'] = df['col1'] + df['col2'] / df['col3']\n",
    "df['colNEW'] = 10\n",
    "df['colNEW'] = np.mean(df['col1'])\n",
    "df['colNEW'] =df['col1'] / 2,52 # inches to centimeters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the colums data types make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "#convert to ints to floats\n",
    "df.['col1'] = df.['col1'].astype(np.float64) #or something else\n",
    "# to string?\n",
    "df.['col2'] = df.['col2'].astype(np.str)\n",
    "#BTW\n",
    "df.[['col1','col2']] = df.[['col1','col2']].astype(np.str) #also works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically however columns show a 'object' where you expect a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'].unique() #will likely indicate problems\n",
    "df.['col1'].value_counts # also does the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function looping over values for correction and type settting\n",
    "def cleaner(x):\n",
    "    x = x.replace('-','') #replace something at the same time\n",
    "    try:\n",
    "        return float(x) #forces float\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "df['col1']= df['col1'].map(cleaner)\n",
    "# df = df.apply(cleaner) will do the WHOLE data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(x):\n",
    "    try: \n",
    "        return x.replace('<', '')\n",
    "    except AttributeError:\n",
    "        return np.NaN\n",
    "df['DLE'] = df['DLE'].map(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or:\n",
    "df['col1'] = df['col1'].map(lambda x: float(x))\n",
    "#or\n",
    "df.loc[:,'col1'] = df['col1'].map(lambda x: 1 if x == 'yes' else 0 if x == 'no' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get into the difficult bit: 'Real' cleaning, namely Na/Null, outliers (including inconsistencies: e.g. a 3 year old owning a car) Consider: Size of the data set, use of the data, the purpose of the analysis. Scan the 'cleaned' data for patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial EDA\n",
    "from pandas_summary import DataFrameSummary\n",
    "dfs = DataFrameSummary(df)\n",
    "dfs.columns_stats # select suspect columns\n",
    "dfs.summary() # inspect the suspects\n",
    "dfs['col1'] # detail the suspects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.select_dtypes(include=['int64'])\n",
    "y = x.describe()\n",
    "y.to_csv('out.csv')\n",
    "y.to_excel('out.xls') # EXCEL ALERT!!! :-(((\n",
    "\n",
    "# 'int', 'float', object, 'datetime'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Or create a standardised box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the numerical columns\n",
    "df_num = df.select_dtypes(exclude=['object'])\n",
    "df_stand = (df - df.np.mean()) / df.np.std()\n",
    "#draw the plot\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax = sns.boxplot(data=df_STAND, orient='h', fliersize=5, \n",
    "                 linewidth=1, notch=True, saturation=0.5, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Na's\n",
    "from numpy import nan as NA\n",
    "df.dropna() #returns rows with any NA\n",
    "df = df.dropna() #removes rows with any NA\n",
    "df = df.dropna(how = 'all') #removes rows with only NA's\n",
    "#the more gentle way:\n",
    "# This will show us records where `df['col1']` is null.\n",
    "mask = df['col1'].isnull()\n",
    "# use the mask to remove those values\n",
    "df = df.loc[~mask,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data entry errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask:\n",
    "mask = df['col1'] == value # <>== etc\n",
    "mask1 = (df['col1']) > value) & (df['col2'] < value)\n",
    "# view rows\n",
    "df.loc[mask,:]\n",
    "#or:\n",
    "df.loc[df['col1'] == value]\n",
    "#decide what to do: override the culprit:\n",
    "df.loc[mask, 'col1'] = np.nan #or override the value\n",
    "df.loc[mask, 'col1'] = new_value/new string\n",
    "#or delete entire row\n",
    "df = df.loc[~mask,:].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Code.isin(['GWA_BTC', 'MWA_BTC_JPY', 'MWA_BTC_EUR']) \n",
    "   & (df.Date == '2018-01-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating rows with outliers in 1 column based on percentile\n",
    "https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.percentile(df['col1'], [5, 95])\n",
    "df = df[(df['col1'] > P[0]) & (df['col1'] < P[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on Mean +/- 3 * std of a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready ?  Save the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('filename.pkl')\n",
    "df = pd.read_pickle('filename.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a kitkat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to print all rows with some defined characterisics\n",
    "# in a df, for inspection\n",
    "# def row_inspector(row):\n",
    "def row_inspector(row):\n",
    "    print ('--------------------------------')\n",
    "    print(row['col1'], row['col2'],row['col3'],\\\n",
    "         'MyComment', row['col3']' < condition)\n",
    "#row_inspector(df.iloc[0,:]) test\n",
    "df.head(X).apply(row_inspector, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIP: knn-classification-imputation-solutions\n",
    "# filling blanks with nearest neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing field int bianvy using lambda\n",
    "y = df['Result'].map(lambda x: 1 if x == 'Play' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.Category == 'ASSUALT' \n",
    "df.loc[mask, ['Category']] = 'ASSAULT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signs that do not seem to be wanted to be deleted \n",
    "df[col] = df[col].astype(str)  # cast to string\n",
    "\n",
    "# all the string surgery goes in here\n",
    "df[col] = df[col].replace('$', '')\n",
    "df[col] = df[col].replace(',', '')  # assuming ',' is the thousand's separator in your locale\n",
    "df[col] = df[col].replace('%', '')\n",
    "\n",
    "df[col] = df[col].astype(float)  # cast back to appropriate types = star[star.Age != '?']\n",
    "s.loc[:,'Age'] = s.Age.map(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminating columns with '?' and resetting to float\n",
    "df = df[df.col1 != '?']\n",
    "df.loc[:,'col1'] = df.col1. map(lambda x: float(x))\n",
    "#or if not thrown away\n",
    "df['col1'] = df['col1'].map(lambda x: np.nan if x == '?' else float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a word count\n",
    "len(x.split())\n",
    "rt['quote_len'] = rt.quote.map(lambda x: len(x.split()))\n",
    "rt = rt[rt.quote_len > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the reviews, making a column with the case and punctuation removed:\n",
    "import string\n",
    "rt['qt'] = rt.quote.map(lambda x: str(\n",
    "    ''.join([y for y in list(x.lower()) if y in string.ascii_lowercase+' -'])))\n",
    "rt.qt = rt.qt.map(lambda x: x.replace('-', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rows\n",
    "rdd2 = rdd1.map(lambda row: row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will filter out a lot of future warnings from statsmodels\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import datetime\n",
    "np.set_printoptions(precision=4)\n",
    "sns.set(font_scale=1.5)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# 1. Create a datetime index\n",
    "data['date'] = pd.to_datetime(data.year_quarter).dt.to_period('Q')\n",
    "data.set_index('date', inplace=True)\n",
    "\n",
    "#2 Visually examine the trend (checck for consistency and completeness)\n",
    "data['unemployment_rate'].plot(lw=4, figsize=(12, 5))\n",
    "plt.show()\n",
    "\n",
    "# let's keep that as a variable for later\n",
    "urate = data.unemployment_rate\n",
    "\n",
    "# MANUAL Acf\n",
    "# Here the acf are calculated as follows:\n",
    "k = 3\n",
    "df_acf = pd.DataFrame({'a':urate[:-k].values,'b':urate[k:].values})\n",
    "\n",
    "numerator = np.sum((df_acf.loc[:,'a']-urate.mean())*(df_acf.loc[:,'b']-urate.mean()))\n",
    "denom = ((urate-urate.mean())**2).sum()\n",
    "numerator/denom\n",
    "\n",
    "# AUTOMATIC Statsmodels\n",
    "\n",
    "print(acf(urate))\n",
    "\n",
    "plot_acf(urate, lags=30, alpha=0.05)# alpha is confidence interval, ie signicant or \n",
    "#just there by chance(anything within contour is 95% certain by chance, outwith could \n",
    "#also be down to chance but only 95%)\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "# calculates the 'Beta' for each added lag, i.e. for each lag the information it adds\n",
    "print(pacf(urate))\n",
    "\n",
    "plot_pacf(urate, lags=30)\n",
    "plt.show()\n",
    "\n",
    "#Let's create a function to display autocorrelations and partial autocorrelations together.\n",
    "\n",
    "def autocorr_plots(y, lags=None):\n",
    "    fig, ax = plt.subplots(ncols=2, figsize=(12, 4), sharey=True)\n",
    "    plot_acf(y, lags=lags, ax=ax[0])\n",
    "    plot_pacf(y, lags=lags, ax=ax[1])\n",
    "    return fig, ax\n",
    "fig, ax = autocorr_plots(urate,lags=20)\n",
    "#if the auto correlations ar low: white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loose stuff to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.rolling(2).mean().plot() #rolling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elegantly subsettinG\n",
    "df = data[data['col1'] == value]\n",
    "dfc = pd.DataFrame(df.groupby(['value'])['newCol'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 date columes into 1\n",
    "df['date'] = df['year'].astype('str')+'/'+df['month']./\n",
    "astype('str')+'/'+df['day'].astype('str')\n",
    "#convert to data time\n",
    "pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A panda series has a (multilevel) index: df.index; the values are in \n",
    "# df.values\n",
    "# If you want to run a function based for a pandas but have a series:\n",
    "# functionX(pd.Series(df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Code.isin(['GWA_BTC', 'MWA_BTC_JPY', 'MWA_BTC_EUR']) \n",
    "   & (df.Date == '2018-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Dummy\" checks for data consistency\n",
    "data_consistency_test = {}\n",
    "\n",
    "# total_nr_trx = nr_debit_trx + nr_credit_trx\n",
    "data_consistency_test[0] = (df['total_nr_trx'] - df['nr_debit_trx'] - df['nr_credit_trx']).sum()\n",
    "\n",
    "#minimum balance is smaller than maximum balance\n",
    "data_consistency_test[1] = (df['min_balance']>df['max_balance']).sum()\n",
    "\n",
    "#volume_credit_trx is 0 when nr_credit_trx is 0\n",
    "data_consistency_test[2] = df[df['volume_credit_trx']==0]['nr_credit_trx'].sum()\n",
    "data_consistency_test[3] = df[df['nr_credit_trx']==0]['volume_credit_trx'].sum()\n",
    "\n",
    "#volume_credit_trx is 0 when nr_credit_trx is 0\n",
    "data_consistency_test[4] = df[df['volume_debit_trx']==0]['nr_debit_trx'].sum()\n",
    "data_consistency_test[5] = df[df['nr_debit_trx']==0]['volume_debit_trx'].sum()\n",
    "\n",
    "\n",
    "assert all(test == 0 for test in data_consistency_test.values())\n",
    "print ('All test have been passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of time series modeling\n",
    "\n",
    "- Visualize the time series\n",
    "- Make the time series stationary\n",
    " - Is there a clear trend in the data?\n",
    " - Are there periodic patterns / seasonal effects?\n",
    "- Plot ACF/PACF to seek optimal parameters\n",
    "- Build the ARIMA model\n",
    "    - Evaluate the model with AIC or BIC \n",
    "- Predict the future\n",
    "    - Obtain in-sample and out-of-sample forecasts\n",
    "    - Evaluate model scores\n",
    "- Make profit (another lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#differnce the data to eliminate the trend #box-cox transformations:google\n",
    "\n",
    "data['unemp_diff'] = data['unemployment_rate'].diff()\n",
    "urate = data.unemployment_rate\n",
    "udiff = data.unemp_diff[1:]\n",
    "\n",
    "data['unemp_diff'].plot(lw=3, figsize=(12, 5), fontsize=16)\n",
    "plt.show()\n",
    "#compare autocorr before and after: you want autocorrs to decay fast: simplere model later;\n",
    "#i.e. only the first few lags are siggnificant \n",
    "fig, ax = autocorr_plots(urate,lags=20)\n",
    "fig, ax = autocorr_plots(udiff,lags=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "ar = ARMA(udiff, (1, 0)).fit(trend='nc') # 1 is the p, i.e. modelling on p preceidng period \n",
    "#if  time series is differnced 'nc' is good\n",
    "# nc stands for having no constant trend\n",
    "# alternative (default) is c for constant\n",
    "# including a constant gives a convergence warning\n",
    "\n",
    "ar.summary2() \n",
    "#use summary2 rather than summary to have properly formatted output\n",
    "#modulus(see report) should be as far from 0 as possible at least larger then 1\n",
    "#if modulus close to 0, likely too many differencing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma1 = ARMA(udiff, (0, 1)).fit()# 1 is q i.e. model on 1 preceding residual\n",
    "ma1.summary2()\n",
    "\n",
    "model = ARMA(udiff, (2,1)).fit(trend='nc') #2 = p 1 = q , aka ARMApq\n",
    "model.summary2()\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA#ARIMA(p d q ) \n",
    "model = ARIMA(urate,order=(2,1,1)).fit(trend='nc')# 2 previous, 1 differencing , \n",
    "#1 previous epsilon, runs on u data so not differnenced/original data\n",
    "model.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having obtained the stationary time series, inspect the autocorrelation and partial autocorrelation plots.\n",
    "# Check the autocorrelations\n",
    "# If all autocorrelations with a lag larger than q vanish, choose MA(q)\n",
    "# If there are autocorrelations at all lags (even if maybe very small), check for the partial autocorrelations\n",
    "# If the partial autocorrelations for lags larger than p vanish, choose AR(p)\n",
    "# If both the ACF and PACF show a gradual decay, an ARMA model is likely appropriate as opposed to the AR or MA alone.\n",
    "# #Model tuning\n",
    "# # Now we can build complex models. We need to tune three values in the ARIMA model:\n",
    "# # AR(p)\n",
    "# # MA(q)\n",
    "# differencing (d)\n",
    "# p indicates how many prior time periods are taken into consideration for explained autocorrelation. Increasing p would increase the dependency on previous values further (longer lag).\n",
    "# q indicates how many prior time periods we are considering for observing sudden trend changes.\n",
    "# d indicates what difference we are anticipating to predict. We pick d in such a way that we produce a stationary time series (if we can).\n",
    "# The best model is usually found automatically\n",
    "# Selection criteria are AIC or BIC (lowest value for given p, q)\n",
    "# AIC stands for Akaike information criterion\n",
    "# BIC stands for Bayesian information criterion\n",
    "# Both are based on the maximum log-likelihood and the number of features\n",
    "# AIC and BIC are relative measures of information gain from our model\n",
    "# AIC and BIC cannot tell us quality in an absolute sense\n",
    "# Parsimonious models are the goal — as few features as possible\n",
    "# AICBIC==2k−2logL̂ ln(n)k−2logL̂ \n",
    "# A\n",
    "# I\n",
    "# C\n",
    "# =\t2k−2log⁡\n",
    "# L\n",
    "# ^\n",
    "# B\n",
    "# I\n",
    "# C\n",
    "# =\tln⁡(n)k−2log⁡\n",
    "# L\n",
    "^\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AR(i)MA can do 'endog' i.e. the time series, as well as exogeneous, \n",
    "#eg. endog = daily ice cream sales, exog is temparature on the same days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic selection of ARMA model based on AIC or BIC\n",
    "from statsmodels.tsa import stattools\n",
    "\n",
    "auto_select = stattools.arma_order_select_ic(\n",
    "    udiff, max_ar=4, max_ma=4, ic=['aic', 'bic'], trend='nc')\n",
    "\n",
    "print(auto_select)\n",
    "\n",
    "sns.heatmap(auto_select['aic'], annot=True, fmt='.1f')\n",
    "plt.title('AIC auto select')\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(auto_select['bic'], annot=True, fmt='.1f')\n",
    "plt.title('BIC auto select')\n",
    "plt.show()\n",
    "\n",
    "# p for AR in the row of the heatmao, q for MA in the column, go \n",
    "#for less complexity, the highest number with the lowest numbers for p and q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fit our best model according to BIC, ARMA(2,3)\n",
    "model = ARMA(udiff,order=(2,3))\n",
    "result = model.fit(start_params=[0,0,0,0,0],solver='lbfgs',method='mle',trend='nc',ic='BIC')\n",
    "# without setting start_params it might not succeed in finding the MLE\n",
    "# fiddle around with the number of zeros in start_params in case of errors\n",
    "# requires as many start_params as model parameters, i.e. p+q\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecast for this model\n",
    "We distinguish different ways of forecasting.\n",
    "In-sample forecasts:\n",
    "We predict one-step ahead using the true values up to that point (non-dynamic forecasting).\n",
    "We use the first few values in the time series to predict the next ones, and the ones further in the future using the predicted values as input (dynamic forecasting).\n",
    "Out-of-sample forecasts:\n",
    "We predict further into the future using predicted values as input.\n",
    "If we predict for a long time into the future, our forecast will be (very close to) the mean.\n",
    "It is important to indicate confidence intervals for our forecast. We can never be sure about the future, but we can be reasonably sure that future values will stay within certain bounds.\n",
    "The width of the confidence interval becomes constant. This is due to stationarity. For a non-stationary time series, the confidence interval would become wider and wider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_1 = 1\n",
    "end_1 = len(udiff)+50\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "result.plot_predict(init_1,end_1,dynamic=False,plot_insample=True,ax=ax)\n",
    "ax.set_title('In-sample predictions and out-of-sample forecasts',fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting and end points for forecasting \n",
    "init_1 = 100\n",
    "end_1 = len(udiff)+50\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2,figsize=(20,8))\n",
    "\n",
    "# ax[0]\n",
    "result.plot_predict(init_1,end_1,dynamic=False,plot_insample=True,ax=ax[0])\n",
    "\n",
    "# The following line is added to the plot just to show\n",
    "# how to obtain the same values\n",
    "result.predict(start=init_1,end=end_1,dynamic=False).plot(\n",
    "    ax=ax[0],color='green',ls='-.',lw=4,alpha=0.5,label='predicted values')\n",
    "\n",
    "ax[0].set_title('Non-dynamic forecasts',fontsize=24)\n",
    "ax[0].legend(fontsize=20)\n",
    "\n",
    "# ax[1]\n",
    "result.plot_predict(init_1,end_1,dynamic=True,plot_insample=True,ax=ax[1])\n",
    "\n",
    "# The following line is added to the plot just to show\n",
    "# how to obtain the same values\n",
    "result.predict(start=init_1,end=end_1,dynamic=True).plot(#dynamic = True takes the \n",
    "    #predicted values rather than the actual values\n",
    "    ax=ax[1],color='green',ls='-.',lw=4,alpha=0.8,label='predicted values')\n",
    "\n",
    "ax[1].set_title('Dynamic forecasts',fontsize=24)\n",
    "ax[1].legend(fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#landa series with the prdited values\n",
    "result.predict(start=init_1,end=end_1,dynamic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model tuning\n",
    "Now we can build complex models. We need to tune three values in the ARIMA model:\n",
    "AR(p)\n",
    "MA(q)\n",
    "differencing (d)\n",
    "p indicates how many prior time periods are taken into consideration for explained autocorrelation. Increasing p would increase the dependency on previous values further (longer lag).\n",
    "q indicates how many prior time periods we are considering for observing sudden trend changes.\n",
    "d indicates what difference we are anticipating to predict. We pick d in such a way that we produce a stationary time series (if we can).\n",
    "The best model is usually found automatically\n",
    "Selection criteria are AIC or BIC (lowest value for given p, q)\n",
    "AIC stands for Akaike information criterion\n",
    "BIC stands for Bayesian information criterion\n",
    "Both are based on the maximum log-likelihood and the number of features\n",
    "AIC and BIC are relative measures of information gain from our model\n",
    "AIC and BIC cannot tell us quality in an absolute sense\n",
    "Parsimonious models are the goal — as few features as possible\n",
    "AICBIC==2k−2logL̂ ln(n)k−2logL̂ \n",
    "A\n",
    "I\n",
    "C\n",
    "=\t2k−2log⁡\n",
    "L\n",
    "^\n",
    "B\n",
    "I\n",
    "C\n",
    "=\tln⁡(n)k−2log⁡\n",
    "L\n",
    "^\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE sklearn:\\t\\t', mean_squared_error(udiff, result.fittedvalues))\n",
    "print('MSE statsmodels:\\t', result.sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to baseline\n",
    "mean_squared_error(udiff, [udiff.mean()] * len(udiff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Dickey-Fuller test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries,maxlag=None,regression='c',autolag='AIC'):\n",
    "    '''Perform Dickey-Fuller test and print out results'''\n",
    "    \n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries,maxlag=maxlag,regression=regression,autolag=autolag)\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in list(dftest[4].items()):\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput) \n",
    "\n",
    "def rolling(timeseries,window=12,center=True,figsize=(12,8)):\n",
    "    '''\n",
    "    Plot original timeseries, \n",
    "    rolling mean over given window size and rolling \n",
    "    mean plus/minus standard deviation\n",
    "    '''\n",
    "    \n",
    "    rolmean = timeseries.rolling(window=window, center=center).mean()\n",
    "    rolstd = timeseries.rolling(window=window, center=center).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std_plus = plt.plot(rolmean+rolstd, color='red',ls='--', label = 'Rolling Mean +/- Rolling Std')\n",
    "    std_minus = plt.plot(rolmean-rolstd, color='red',ls='--',label= '')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()  \n",
    "\n",
    "def autocorrelation_plot(timeseries,lags=None):\n",
    "    '''Plot autocorrelations and partial autocorrelations'''\n",
    "    fig, ax = plt.subplots(ncols=2,figsize=(12,4))\n",
    "    plot_acf(timeseries, lags=lags, ax=ax[0])\n",
    "    plot_pacf(timeseries, lags=lags, ax=ax[1])\n",
    "    plt.show()\n",
    "\n",
    "def test_and_vis(timeseries):\n",
    "    '''\n",
    "    Perform Dickey-Fuller test, \n",
    "    plot timeseries with rolling mean and autocorrelations\n",
    "    '''\n",
    "    test_stationarity(timeseries.dropna())\n",
    "    rolling(timeseries)\n",
    "    autocorrelation_plot(timeseries.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_vis(bus.riders) # null hyputhesis not stationary, so below 0.05 accept if\n",
    "#high reject ie.e not stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose\n",
    "df0 =df0.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Piece of code that did not work well, decide to go drop_duplicates later\n",
    "\n",
    "# column = df['DL'].values\n",
    "\n",
    "# drop_values = []\n",
    "# for i,value in enumerate(column):\n",
    "#     if column[i]==column[i-1] and column[i] in ('1','2','3','4','5','6','7','8', '9', '<','>'):\n",
    "#         value = [i-1]\n",
    "#         drop_values.append(value)\n",
    "\n",
    "\n",
    "# len(drop_values)\n",
    "\n",
    "# df.shape\n",
    "\n",
    "# np.array(drop_values).ravel()\n",
    "\n",
    "# df[15:35]\n",
    "\n",
    "# for i in np.array(drop_values).ravel():\n",
    "#     df.drop(i, inplace=True)\n",
    "\n",
    "# df[15:35]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
